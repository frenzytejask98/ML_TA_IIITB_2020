{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the steps involved to understand, clean and prepare your data for building your predictive model:\n",
    "\n",
    "   1.  Variable Identification\n",
    "   2.  Missing values treatment\n",
    "   3.  Outlier treatment\n",
    "   4.  Encoding\n",
    "   5.  A little Viz\n",
    "   \n",
    "Finally, we will need to iterate over steps 4 â€“ 7 multiple times before we come up with our refined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Minmax scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house=pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Identification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify Predictor (Input) and Target (output) variables \n",
    "\n",
    "For example for the given dataset, problem statements could be, \n",
    "\n",
    "1. Given the information of the patient what is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Seperate them into Datatypes of catogarical and Continuouse ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = house.dtypes[house.dtypes == 'object'].index\n",
    "house[a].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values\n",
    "We treat missing values in a variety of ways, \n",
    "\n",
    "    1. Deletion\n",
    "    2. Mean/Mode/Median imputation\n",
    "    3. Prediction Model: In this we train a model that is let to overfit to predict the missing values. \n",
    "    \n",
    "What we use is subjective and depends on the use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.Street.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.Alley.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.Alley.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.drop('Alley',inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = house.dtypes[house.dtypes == 'object'].index\n",
    "house[a].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house[a].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.BsmtCond.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = house.dtypes[house.dtypes == 'float64'].index\n",
    "house[b].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.MasVnrArea.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.GarageYrBlt.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = house.dtypes[house.dtypes == 'int64'].index\n",
    "house[c].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house[a].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house[['MasVnrType', 'BsmtQual', 'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2', 'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see PoolQC,Fence,MiscFeatures for example have many nan values but they definitely add a lot of value in determining the house prices and hence removing the column doesn't make sense. It can safely be said that the NaN values here maybe a No. So we will use a fillna('No')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out ffill and bfill methods in fillna. They are usually used in timeseries/order sensitive data where missing values could be easily identified and replaced based on trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.fillna({'MasVnrType':'No', 'BsmtQual':'No', 'BsmtCond':'No','BsmtExposure':'No','BsmtFinType1':'No','BsmtFinType2':'No', 'FireplaceQu':'No','GarageType':'No','GarageFinish':'No','GarageQual':'No','GarageCond':'No','PoolQC':'No','Fence':'No','MiscFeature':'No'}, inplace=True)\n",
    "house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.Electrical.value_counts()\n",
    "#Replace with mode\n",
    "house['Electrical'].fillna(\"SBrkr\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, LotFrontage has a lot of null values, around 259 of em. The way we go about filling these values can go in various directions. Let's see. The null values might be because they are essentially not attached to any street inherently, so it might make sense to replace them with 0. But, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house[house.LotFrontage.isnull()].LotConfig.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that that might not be the case. So we need to fill in some sensible value here. One direction of thinking might be that houses in a specific neighborhood will have similar LofFrontage due to their arrangement, so we substitute the nulls values with the mean of LotFrontage of various neighbourhoods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house[['Neighborhood', 'LotFrontage']].groupby(['Neighborhood']).transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Or we can even do better! Houses in the corners and Cul-De-Sac (meaning at the end of the road) etc will have different LotFrontage due to the property type, so it might make even more sense to group them based on neighbour hood and LotConfig, and then use the means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house[['Neighborhood', 'LotFrontage', 'LotConfig']].groupby(['Neighborhood', 'LotConfig']).transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is better? Well, the latter seems more accurate but you'll know if your approach makes sense once you train and see the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skew Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(house.LotArea, bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(house.LotArea), bins = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurtosis is a measure of tailed-ness or spread of the distribution. mathematically it is calculated using the expectation of the 4 degree of variation from the mean, which would be E((X - E[X])^4), divided by the 4th power of standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.LotArea.kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tailed-ness needs to be removed, as the tailed-ness is due to outliers. We can use the box plot concept to identify the outliers, and then replace them with mean/median, or use winsorisation (use the extremum values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(x):\n",
    "    q1 = house.LotArea.quantile(q = 0.25)\n",
    "    q3 = house.LotArea.quantile(q = 0.75)\n",
    "    iqr = q3 - q1\n",
    "    outlier_range = 1.5*iqr\n",
    "    r_whisker = q3 + outlier_range\n",
    "    l_whisker = q1 - outlier_range\n",
    "    if (x > r_whisker):\n",
    "        return q3\n",
    "    elif (x < l_whisker):\n",
    "        return q1\n",
    "    else: \n",
    "        return x\n",
    "    \n",
    "house.LotArea.apply(lambda x: remove_outliers(x)).kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has decreased the kurtosis a lot, but we are loosing a lot of original values. So it might be best to test both cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Values\n",
    "\n",
    "Sometimes, the numbers might not make sense as numbers. Sometimes the columns won't have numbers at all. We need to handle them well as your model can only understand numbers.\n",
    "\n",
    "Categories mainly come as two types, ordinal and non-ordinal. Ordinal values simply have a inherent order in the categories, like \"low, medium, high\", years and months. Non-ordinal would be values which have no inherent order in them like countries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some values might might make more sense as binned categories than their intrinsic numerical values. For example time. Years and months are represented as numbers but they aren't numbers, so it makes more sense to label encode them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.YrSold = house.YrSold.astype(str)\n",
    "house.MoSold = house.MoSold.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some numerical values are very discreet, and it makes more sense to bin them into respective categories, like MSSubClass which is the class of the building and the value here is some type of a code than a number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.MSSubClass.value_counts()\n",
    "# house.MSSubClass = house.MSSubClass.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to treat them as categories, we need to either label encode them or one-hot encode them. \n",
    "\n",
    "Label-encoding simply gives them values like 0,1,2.... whereas one-hot encoding takes each category as a column and throw a value 1 for positive hit and 0 for a negative hit. There is an issue with label-encoding, as you are giving orderly values to categorical data which might not even be ordered, effectively changing the data and the model's understanding of the data. In such case we use the one-hot encoding method, but even this has an issue of multi-collinearity, meaning correlation between various columns. How? Well, if you find a 1 in one of the columns, it would mean that the other columns will definitely be 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding\n",
    "lencoder = LabelEncoder() \n",
    "lencoder.fit_transform(house.MSSubClass) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.MSSubClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't make sense as the column isn't actually ordered. So we'll use one-hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumm = pd.get_dummies(house.MSSubClass)\n",
    "# pd.concat([house, dumm], axis = 1)\n",
    "# house.drop(columns=[['MSSubClass']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how do you ensure multicollinearity isn't a problem? We have a statistical measure that we can check. It's called variance inflation factor (VIF). Due to time constraint, we recommend you read [this](https://online.stat.psu.edu/stat462/node/180/) for VIF. Put your queries on slack if you have any doubts.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(data):\n",
    "    vif_df = pd.DataFrame(columns = ['Var', 'Vif'])\n",
    "    x_var_names = data.columns\n",
    "    for i in range(0, x_var_names.shape[0]):\n",
    "        y = data[x_var_names[i]]\n",
    "        x = data[x_var_names.drop([x_var_names[i]])]\n",
    "        r_squared = sm.OLS(y,x).fit().rsquared\n",
    "        vif = round(1/(1-r_squared),2)\n",
    "        vif_df.loc[i] = [x_var_names[i], vif]\n",
    "    return vif_df.sort_values(by = 'Vif', axis = 0, ascending=False, inplace=False)\n",
    "\n",
    "vifcalc = pd.get_dummies(house.MSSubClass)\n",
    "calculate_vif(vifcalc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we do now? we drop the dummy columns which have very high VIF (above 5). After that, recalculate the VIF for all columns and you'll see a drop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we use? There are some heuristics you can follow, \n",
    "1. Use one-hot encoding if the number of categories are low, so you don't end up overloading your data and increasing memory required for training, else use label encoding. \n",
    "2. Use label encoding if the data is inherently ordered (ordinal categories). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualising Categorical values\n",
    "\n",
    "1. A simple bar plot will give you a really good picture of what's happening. If you want to visualise on a condition, like a groupby, you can use grouped bar plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot(house.Neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "sns.countplot(data=house, x= 'Neighborhood', hue = 'LotConfig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
